---
# Libvirt bootstrap tasks - creates and configures VMs
- name: Ensure image directory exists
  ansible.builtin.file:
    path: "{{ image_dir }}"
    state: directory
    mode: '0755'
  become: true

- name: Check if base image already exists
  ansible.builtin.stat:
    path: "{{ image_dir }}/{{ base_image_name }}"
  register: libvirt_base_image_stat

- name: Download Fedora Cloud image
  ansible.builtin.get_url:
    url: "{{ fedora_cloud_image_url }}"
    dest: "{{ image_dir }}/{{ base_image_name }}"
    mode: '0644'
  become: true
  when: not libvirt_base_image_stat.stat.exists
  register: libvirt_download_result

- name: Display download status  # noqa: no-handler
  ansible.builtin.debug:
    msg: "Fedora Cloud image downloaded successfully"
  when: libvirt_download_result is changed

- name: Check existing VMs
  community.libvirt.virt:
    command: list_vms
  register: libvirt_existing_vms

- name: Stop and destroy existing lab VMs
  community.libvirt.virt:
    name: "{{ item.name }}"
    state: destroyed
  loop: "{{ vms }}"
  when: item.name in libvirt_existing_vms.list_vms
  register: libvirt_destroy_result
  failed_when: libvirt_destroy_result.failed and 'not found' not in (libvirt_destroy_result.msg | default(''))

- name: Undefine existing lab VMs
  community.libvirt.virt:
    name: "{{ item.name }}"
    command: undefine
  loop: "{{ vms }}"
  when: item.name in libvirt_existing_vms.list_vms
  register: libvirt_undefine_result
  failed_when: libvirt_undefine_result.failed and 'not found' not in (libvirt_undefine_result.msg | default(''))

- name: Remove old VM disk images
  ansible.builtin.file:
    path: "{{ image_dir }}/{{ item.name }}.qcow2"
    state: absent
  become: true
  loop: "{{ vms }}"

- name: Create VM disk images from base image
  ansible.builtin.command: >
    qemu-img create -f qcow2 -F qcow2
    -b {{ image_dir }}/{{ base_image_name }}
    {{ image_dir }}/{{ item.name }}.qcow2
    {{ item.disk_size }}G
  become: true
  loop: "{{ vms }}"
  changed_when: true

- name: Set permissions on VM disk images
  ansible.builtin.file:
    path: "{{ image_dir }}/{{ item.name }}.qcow2"
    owner: qemu
    group: qemu
    mode: '0600'
  become: true
  loop: "{{ vms }}"

- name: Create cloud-init meta-data files
  ansible.builtin.copy:
    content: |
      instance-id: {{ item.name }}
      local-hostname: {{ item.hostname }}
    dest: "/tmp/{{ item.name }}-meta-data"
    mode: '0644'
  loop: "{{ vms }}"

- name: Create cloud-init user-data files
  ansible.builtin.copy:
    content: |
      #cloud-config
      hostname: {{ item.hostname }}
      fqdn: {{ item.hostname }}
      manage_etc_hosts: true

      users:
        - name: {{ cloud_user }}
          sudo: ALL=(ALL) NOPASSWD:ALL
          groups: wheel
          shell: /bin/bash
          ssh_authorized_keys:
            - {{ ssh_public_key_content }}

      # Change default target to multi-user (no GUI)
      bootcmd:
        - systemctl set-default multi-user.target

      # Ensure SSH is enabled
      runcmd:
        - systemctl enable sshd
        - systemctl start sshd

      # Grow root filesystem
      growpart:
        mode: auto
        devices: ['/']

      packages:
        - qemu-guest-agent

      package_update: true
      package_upgrade: false

      final_message: "Cloud-init completed. System is ready."
    dest: "/tmp/{{ item.name }}-user-data"
    mode: '0644'
  loop: "{{ vms }}"

- name: Create cloud-init ISO images
  ansible.builtin.command: >
    genisoimage -output {{ image_dir }}/{{ item.name }}-cidata.iso
    -volid cidata -joliet -rock
    /tmp/{{ item.name }}-user-data /tmp/{{ item.name }}-meta-data
  become: true
  loop: "{{ vms }}"
  changed_when: true

- name: Set permissions on cloud-init ISOs
  ansible.builtin.file:
    path: "{{ image_dir }}/{{ item.name }}-cidata.iso"
    owner: qemu
    group: qemu
    mode: '0600'
  become: true
  loop: "{{ vms }}"

- name: Get libvirt default network configuration
  community.libvirt.virt_net:
    command: get_xml
    name: "{{ libvirt_network }}"
  register: libvirt_default_network_xml

- name: Parse network XML
  community.general.xml:
    xmlstring: "{{ libvirt_default_network_xml.get_xml }}"
    xpath: /network/ip/dhcp/host[@mac='{{ item.mac }}']
    count: true
  loop: "{{ vms }}"
  register: libvirt_dhcp_host_check

- name: Stop libvirt default network
  community.libvirt.virt_net:
    name: "{{ libvirt_network }}"
    state: inactive
  when: libvirt_dhcp_host_check.results | selectattr('count', 'equalto', 0) | list | length > 0

- name: Add static DHCP entries to default network
  ansible.builtin.command: >
    virsh net-update {{ libvirt_network }} add ip-dhcp-host
    "<host mac='{{ item.mac }}' name='{{ item.hostname }}' ip='{{ item.ip }}'/>"
    --live --config
  loop: "{{ vms }}"
  when: libvirt_dhcp_host_check.results[loop.index0].count == 0
  register: libvirt_dhcp_update
  changed_when: libvirt_dhcp_update.rc == 0
  failed_when: libvirt_dhcp_update.rc != 0 and 'already exists' not in libvirt_dhcp_update.stderr

- name: Start libvirt default network
  community.libvirt.virt_net:
    name: "{{ libvirt_network }}"
    state: active
  when: libvirt_dhcp_host_check.results | selectattr('count', 'equalto', 0) | list | length > 0

- name: Create VMs using virt-install
  ansible.builtin.command: >
    virt-install
    --name {{ item.name }}
    --memory {{ item.memory }}
    --vcpus {{ item.vcpus }}
    --disk path={{ image_dir }}/{{ item.name }}.qcow2,format=qcow2,bus=virtio
    --disk path={{ image_dir }}/{{ item.name }}-cidata.iso,device=cdrom
    --os-variant fedora-unknown
    --network network={{ libvirt_network }},mac={{ item.mac }}
    --graphics none
    --noautoconsole
    --import
  loop: "{{ vms }}"
  become: true
  changed_when: true

- name: Wait for VMs to be running
  community.libvirt.virt:
    name: "{{ item.name }}"
    command: status
  loop: "{{ vms }}"
  register: libvirt_vm_status
  until: libvirt_vm_status.status == "running"
  retries: 10
  delay: 5

- name: Wait for SSH to be available on VMs
  ansible.builtin.wait_for:
    host: "{{ item.ip }}"
    port: 22
    delay: 10
    timeout: 300
  loop: "{{ vms }}"

- name: Wait additional time for cloud-init to complete
  ansible.builtin.pause:
    seconds: 30
  when: libvirt_download_result is changed

- name: Test SSH connectivity to cloud-user
  ansible.builtin.command: >
    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
    -i {{ ssh_key_path }}
    {{ cloud_user }}@{{ item.ip }}
    'echo SSH connection successful'
  loop: "{{ vms }}"
  register: libvirt_ssh_test
  until: libvirt_ssh_test.rc == 0
  retries: 5
  delay: 10
  changed_when: false

- name: Create ansible user on VMs
  ansible.builtin.command: >
    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
    -i {{ ssh_key_path }}
    {{ cloud_user }}@{{ item.ip }}
    'sudo useradd -m -G wheel -s /bin/bash {{ ansible_user }}'
  loop: "{{ vms }}"
  register: libvirt_user_creation
  failed_when: libvirt_user_creation.rc != 0 and 'already exists' not in libvirt_user_creation.stderr
  changed_when: libvirt_user_creation.rc == 0

- name: Create .ssh directory for ansible user
  ansible.builtin.command: >
    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
    -i {{ ssh_key_path }}
    {{ cloud_user }}@{{ item.ip }}
    'sudo mkdir -p /home/{{ ansible_user }}/.ssh && sudo chmod 700 /home/{{ ansible_user }}/.ssh'
  loop: "{{ vms }}"
  changed_when: true

- name: Copy SSH key to ansible user
  ansible.builtin.command: >
    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
    -i {{ ssh_key_path }}
    {{ cloud_user }}@{{ item.ip }}
    'echo "{{ ssh_public_key_content }}" | sudo tee /home/{{ ansible_user }}/.ssh/authorized_keys'
  loop: "{{ vms }}"
  changed_when: true

- name: Set permissions on ansible user SSH directory
  ansible.builtin.command: >
    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
    -i {{ ssh_key_path }}
    {{ cloud_user }}@{{ item.ip }}
    'sudo chmod 600 /home/{{ ansible_user }}/.ssh/authorized_keys && sudo chown -R {{ ansible_user }}:{{ ansible_user }} /home/{{ ansible_user }}/.ssh'
  loop: "{{ vms }}"
  changed_when: true

- name: Configure passwordless sudo for ansible user
  ansible.builtin.command: >
    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
    -i {{ ssh_key_path }}
    {{ cloud_user }}@{{ item.ip }}
    'echo "{{ ansible_user }} ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/{{ ansible_user }}'
  loop: "{{ vms }}"
  changed_when: true

- name: Set permissions on sudoers file
  ansible.builtin.command: >
    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
    -i {{ ssh_key_path }}
    {{ cloud_user }}@{{ item.ip }}
    'sudo chmod 440 /etc/sudoers.d/{{ ansible_user }}'
  loop: "{{ vms }}"
  changed_when: true

- name: Verify ansible user SSH access
  ansible.builtin.command: >
    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
    -i {{ ssh_key_path }}
    {{ ansible_user }}@{{ item.ip }}
    'sudo echo "Ansible user setup successful"'
  loop: "{{ vms }}"
  changed_when: false

- name: Update /etc/hosts with VM entries
  ansible.builtin.lineinfile:
    path: /etc/hosts
    regexp: ".*{{ item.hostname }}$"
    line: "{{ item.ip }} {{ item.hostname }} {{ item.name }}"
    state: present
  become: true
  loop: "{{ vms }}"

- name: Display completion message
  ansible.builtin.debug:
    msg: |
      ========================================
      Lab environment bootstrap completed!
      ========================================

      VMs created:
      {% for vm in vms %}
      - {{ vm.name }}: {{ vm.ip }} ({{ vm.hostname }})
      {% endfor %}

      SSH key: {{ ssh_key_path }}

      Next steps:
      1. Test connectivity: ansible -i inventory demoservers -m ping
      2. Run provision playbook: ansible-playbook provision-vms.yml
      3. Run site playbook: ansible-playbook site.yml

      To access VMs:
      ssh -i {{ ssh_key_path }} ansible@labhost1.local
      ssh -i {{ ssh_key_path }} ansible@labhost2.local
      ssh -i {{ ssh_key_path }} ansible@labhost3.local
